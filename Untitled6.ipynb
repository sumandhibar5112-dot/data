{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.What is the difference between AI, ML, DL, and Data Science? Provide a\n",
        "brief explanation of each.\n",
        "(Hint: Compare their scope, techniques, and applications for each.)\n",
        "\n",
        "The relationship between Artificial Intelligence (AI), Machine Learning (ML), and Deep Learning (DL) is hierarchical: AI is the overarching field, ML is a subset of AI, and DL is a specialized subset of ML. Data Science is a separate, multidisciplinary field that often uses ML and DL techniques\n",
        "\n",
        "Artificial Intelligence (AI)\n",
        "Explanation: AI is the simulation of human intelligence in machines that are programmed to think and act like humans. It's the big-picture goal of creating \"smart\" computers.\n",
        "\n",
        "\n",
        "Key Idea: It aims to solve cognitive problems (e.g., learning, reasoning, perception) that are typically done by humans\n",
        "\n",
        "Machine Learning (ML)\n",
        "Explanation: ML is an approach or a subset of AI that allows systems to learn from data and make predictions or decisions without being explicitly programmed for every scenario. Instead of writing code with specific rules, you feed the machine data and let it develop its own rules (models).\n",
        "\n",
        "\n",
        "Key Idea: The performance of the system improves with more data and experience\n",
        "\n",
        "Deep Learning (DL)\n",
        "Explanation: DL is a subset of ML that uses deep Artificial Neural Networks (ANNs) with multiple layers, inspired by the structure and function of the human brain. The \"deep\" refers to the number of layers in the network.\n",
        "\n",
        "\n",
        "Key Idea: DL automatically learns features from raw, complex, and large amounts of data, which makes it particularly effective for tasks involving unstructured data like images, video, and audio\n",
        "\n",
        "Data Science (DS)\n",
        "Explanation: Data Science is a multidisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from data in various forms (structured or unstructured). It’s an end-to-end process focused on solving real-world problems.\n",
        "\n",
        "\n",
        "Key Idea: It involves everything from data collection, cleaning, statistical analysis, and visualization to applying ML/DL models and communicating the final insights to make informed decisions."
      ],
      "metadata": {
        "id": "juuG3ZCE8_69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Question 2: Explain overfitting and underfitting in ML. How can you detect and prevent\n",
        "them?\n",
        "Hint: Discuss bias-variance tradeoff, cross-validation, and regularization techniques.\n",
        "\n",
        "Overfitting\n",
        "Overfitting occurs when a model learns the training data too well, including the noise, outliers, and irrelevant details.\n",
        "\n",
        "Result: The model achieves very low error (or high accuracy) on the training data but performs poorly (high error) on new, test data.\n",
        "\n",
        "Cause: The model is typically too complex for the available data.\n",
        "\n",
        "Bias-Variance Tradeoff: Characterized by low bias (it fits the training data closely) and high variance (its predictions change significantly with minor changes in the training data).\n",
        "\n",
        "Underfitting\n",
        "Underfitting occurs when a model is too simple to capture the underlying structure and patterns in the training data.\n",
        "\n",
        "Result: The model performs poorly (high error) on both the training data and the test data.\n",
        "\n",
        "Cause: The model is typically not complex enough or hasn't been trained for a sufficient amount of time.\n",
        "\n",
        "Bias-Variance Tradeoff: Characterized by high bias (it makes strong, incorrect assumptions about the data) and typically low variance (it's insensitive to changes in the training data).\n",
        "\n",
        "Detection and Prevention\n",
        "Detection\n",
        "The primary method for detecting both is by comparing the model's performance on the training set versus an independent validation/test set."
      ],
      "metadata": {
        "id": "DLuDAcTi-x-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3:How would you handle missing values in a dataset? Explain at least three\n",
        "methods with examples.\n",
        "Hint: Consider deletion, mean/median imputation, and predictive modeling.\n",
        "\n",
        "Missing values are common in datasets and can affect the quality of data analysis or machine learning models. There are several methods to handle them, depending on the nature and amount of missing data. Below are three commonly used methods:\n",
        "\n",
        "1. Deletion Methods\n",
        "\n",
        "a) Listwise Deletion (Complete Case Analysis):\n",
        "\n",
        "Description: Remove all rows that contain any missing values.\n",
        "\n",
        "When to use: When the dataset is large and the number of missing values is small (so you don’t lose much data).\n",
        "\n",
        "2. Imputation Methods\n",
        "\n",
        "a) Mean/Median/Mode Imputation:\n",
        "\n",
        "Description: Replace missing values with a central tendency measure (mean, median, or mode) of the column.\n",
        "\n",
        "When to use: When missing values are random and the variable is numeric (for mean/median) or categorical (for mode).\n",
        "\n",
        "3. Predictive Modeling\n",
        "\n",
        "a) Using Machine Learning Models to Predict Missing Values:\n",
        "\n",
        "Description: Treat the column with missing values as a target variable and use other columns to predict it.\n",
        "\n",
        "When to use: When the dataset is large and relationships between variables are strong.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9KXYmx4p-9A6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4:What is an imbalanced dataset? Describe two techniques to handle it\n",
        "(theoretical + practical).\n",
        "Hint: Discuss SMOTE, Random Under/Oversampling, and class weights in models.\n",
        "\n",
        "An imbalanced dataset is a dataset in a classification problem where the number of observations for one class (majority class) significantly outweighs the number of observations for the other class or classes (minority class). This disproportionate distribution causes standard machine learning models to be biased toward the majority class, leading to poor predictive performance for the minority class, which is often the class of interest (e.g., fraud or disease).1. SMOTE (Synthetic Minority Over-sampling Technique)Category: Data Resampling (Oversampling)AspectDescriptionTheoreticalSMOTE is a sophisticated oversampling method that generates synthetic instances for the minority class. Instead of duplicating existing minority data points, it selects a minority observation and its $k$ nearest minority neighbors. It then creates a new synthetic sample at a random point along the line segment connecting the original observation and one of its randomly selected neighbors in the feature space. This helps create a more diverse and representative dataset for the minority class, mitigating the risk of overfitting associated with simple random oversampling.PracticalIn Python, the imblearn library is used. You import SMOTE, initialize it, and then apply it to the training data. The goal is often to balance the classes to a 1:1 ratio.\n",
        "2. Adjusting Class Weights in ModelsCategory: Algorithm Modification (Cost-Sensitive Learning)4AspectDescriptionTheoreticalThis method modifies the loss function of the classification model during training. It assigns a higher weight to the minority class and a lower weight to the majority class. The model is then heavily penalized for misclassifying a minority instance (a False Negative), forcing the optimization algorithm (like Gradient Descent) to prioritize correct prediction of the minority class. A common weighting scheme is inverse class frequency, where the weight is inversely proportional to the class size.PracticalMany machine learning algorithms in scikit-learn (e.g., LogisticRegression, RandomForestClassifier) have a built-in class_weight parameter. Setting this parameter to 'balanced' automatically computes the inverse class weights, making it a simple, effective solution that doesn't alter the original data.\n",
        "\n"
      ],
      "metadata": {
        "id": "YTjFkV07AbJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Why is feature scaling important in ML? Compare Min-Max scaling and\n",
        "Standardization.\n",
        "Hint: Explain impact on distance-based algorithms (e.g., KNN, SVM) and gradient\n",
        "descent.\n",
        "\n",
        "Feature scaling is a crucial data preprocessing step that standardizes or normalizes the range of independent variables or features of the data. It ensures that no single feature dominates the model simply because of its magnitude, thereby ensuring a balanced and fair contribution from all features.\n",
        "\n",
        "Importance of Feature Scaling in ML\n",
        "Feature scaling is essential for two main categories of machine learning algorithms:\n",
        "\n",
        "1. Distance-Based Algorithms (e.g., KNN, SVM, K-Means)\n",
        "Algorithms that rely on Euclidean distance or other distance metrics are heavily influenced by the magnitude of the features.\n",
        "\n",
        "Problem without Scaling: If one feature, like 'Annual Income' (ranging from $10,000 to $100,000), is used alongside 'Age' (ranging from 18 to 70), the distance calculation will be overwhelmingly dominated by the 'Annual Income' feature.\n",
        "\n",
        "Impact: The model will become biased toward the feature with the largest range, effectively ignoring the contribution of the smaller-ranged features, leading to poor predictive performance.\n",
        "\n",
        "2. Gradient Descent Optimization (e.g., Linear Regression, Logistic Regression, Neural Networks)\n",
        "Algorithms that use Gradient Descent to minimize the cost function benefit greatly from scaling.\n",
        "\n",
        "Problem without Scaling: When features have vastly different scales, the cost function forms a very elongated, elliptical contour shape (like a \"tall, skinny bowl\"). Gradient Descent will have to take small steps and will oscillate back and forth along the steep sides, making it slow and inefficient to reach the minimum point.\n",
        "\n",
        "Impact: Scaling the features brings them to a similar range, which results in a more spherical or well-conditioned contour shape. This allows Gradient Descent to take a more direct path, resulting in faster convergence and shorter training times.\n",
        "\n",
        "| **Method**                                   | **Formula**                                      | **Range / Mean**        | **When to Use**                                                           |\n",
        "| -------------------------------------------- | ------------------------------------------------ | ----------------------- | ------------------------------------------------------------------------- |\n",
        "| **1. Min–Max Normalization (Normalization)** | ( X' = \\frac{X - X_{min}}{X_{max} - X_{min}} )   | [0, 1]                  | When you need all values within a fixed range (e.g., Neural Networks).    |\n",
        "| **2. Standardization (Z-Score Scaling)**     | ( X' = \\frac{X - \\mu}{\\sigma} )                  | Mean = 0, Std = 1       | When data follows a Gaussian distribution (e.g., Linear Regression, SVM). |\n",
        "| **3. Robust Scaling**                        | ( X' = \\frac{X - \\text{median}(X)}{\\text{IQR}} ) | Based on median and IQR | When data has many outliers.                                              |\n",
        "\n"
      ],
      "metadata": {
        "id": "diPcX5BHAsuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Compare Label Encoding and One-Hot Encoding. When would you prefer\n",
        "one over the other?\n",
        "Hint: Consider categorical variables with ordinal vs. nominal relationships.\n",
        "\n",
        "Categorical variables often need to be converted into numerical form before being used in machine learning models, since most algorithms require numeric input. Two common methods for this are Label Encoding and One-Hot Encoding.\n",
        "\n",
        "1. Label Encoding\n",
        "\n",
        "Definition:\n",
        "Label Encoding assigns each unique category in a feature an integer value.\n",
        "\n",
        "Example:\n",
        "\n",
        "Color\tEncoded\n",
        "Red\t0\n",
        "Blue\t1\n",
        "Green\t2\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Simple and memory efficient.\n",
        "\n",
        "Keeps data in a single column.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Imposes an ordinal relationship (e.g., Green > Blue > Red), which may mislead the model if the variable is nominal (no natural order).\n",
        "\n",
        "Not suitable for algorithms sensitive to magnitude (e.g., Linear Regression, KNN).\n",
        "\n",
        "When to Use:\n",
        "✅ For ordinal categorical variables — those with a meaningful order but no fixed numeric distance.\n",
        "Example: Education Level → {High School < Bachelor’s < Master’s < PhD}.\n",
        "\n",
        "2. One-Hot Encoding\n",
        "\n",
        "Definition:\n",
        "One-Hot Encoding creates a new binary column (0/1) for each category.\n",
        "\n",
        "Example:\n",
        "\n",
        "Color\tRed\tBlue\tGreen\n",
        "Red\t1\t0\t0\n",
        "Blue\t0\t1\t0\n",
        "Green\t0\t0\t1\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Does not assume any ordinal relationship.\n",
        "\n",
        "Ideal for nominal variables (purely categorical with no order).\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Increases the number of features (especially when categories are many).\n",
        "\n",
        "May cause “curse of dimensionality” in high-cardinality features.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iq933gXABErN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Analyze the relationship between app categories and ratings. Which categories have the\n",
        "#highest/lowest average ratings, and what could be the possible reasons?\n",
        "#Dataset: https://github.com/MasteriNeuron/datasets.git\n",
        "#(Include your Python code and output in the code box below.)\n",
        "import pandas as pd\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "import altair as alt\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"googleplaystore.csv\")\n",
        "\n",
        "# Drop rows where 'Rating' is null\n",
        "df_cleaned = df.dropna(subset=['Rating']).copy()\n",
        "\n",
        "# Check for max value in Rating and handle the anomaly\n",
        "max_rating = df_cleaned['Rating'].max()\n",
        "if max_rating > 5.0:\n",
        "    # Remove the outlier row (which is the row with '19' in the Category column)\n",
        "    df_cleaned = df_cleaned[df_cleaned['Rating'] <= 5.0]\n",
        "\n",
        "# Calculate the average rating for each category\n",
        "category_ratings = df_cleaned.groupby('Category')['Rating'].mean().sort_values(ascending=False)\n",
        "\n",
        "# Get the top 5 and bottom 5 categories\n",
        "top_5_categories = category_ratings.head(5)\n",
        "bottom_5_categories = category_ratings.tail(5)\n",
        "\n",
        "print(\"Top 5 Categories by Average Rating:\")\n",
        "print(top_5_categories.to_markdown(numalign=\"left\", stralign=\"left\"))\n",
        "print(\"\\nBottom 5 Categories by Average Rating:\")\n",
        "print(bottom_5_categories.to_markdown(numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "# Prepare data for plotting\n",
        "plot_df = pd.concat([top_5_categories, bottom_5_categories]).reset_index()\n",
        "plot_df.columns = ['Category', 'Average_Rating']\n",
        "\n",
        "# Create the Altair chart (JSON saved in previous execution)\n",
        "chart = alt.Chart(plot_df).mark_bar().encode(\n",
        "    x=alt.X('Average_Rating', title='Average Rating', axis=alt.Axis(format=\".2f\", tickMinStep=0.05)),\n",
        "    y=alt.Y('Category', sort='x', title='App Category'),\n",
        "    color=alt.condition(\n",
        "        alt.datum.Average_Rating >= top_5_categories.min(),\n",
        "        alt.value('darkgreen'),\n",
        "        alt.value('darkred')\n",
        "    ),\n",
        "    tooltip=['Category', alt.Tooltip('Average_Rating', format=\".3f\")]\n",
        ").properties(\n",
        "    title='Top 5 and Bottom 5 App Categories by Average Rating'\n",
        ").interactive()\n",
        "chart.save('category_ratings_bar_chart.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C11E7xP1XMN-",
        "outputId": "e948e328-cf14-4a81-bfa6-b22f42535096"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Categories by Average Rating:\n",
            "| Category            | Rating   |\n",
            "|:--------------------|:---------|\n",
            "| EVENTS              | 4.43556  |\n",
            "| EDUCATION           | 4.38903  |\n",
            "| ART_AND_DESIGN      | 4.35806  |\n",
            "| BOOKS_AND_REFERENCE | 4.34607  |\n",
            "| PERSONALIZATION     | 4.33599  |\n",
            "\n",
            "Bottom 5 Categories by Average Rating:\n",
            "| Category            | Rating   |\n",
            "|:--------------------|:---------|\n",
            "| LIFESTYLE           | 4.0949   |\n",
            "| VIDEO_PLAYERS       | 4.06375  |\n",
            "| MAPS_AND_NAVIGATION | 4.05161  |\n",
            "| TOOLS               | 4.04741  |\n",
            "| DATING              | 3.97077  |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c837520",
        "outputId": "22b6d98a-1347-4580-951e-130148e347be"
      },
      "source": [
        "# Fetch the dataset from the GitHub link\n",
        "!wget https://raw.githubusercontent.com/MasteriNeuron/datasets/main/googleplaystore.csv"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-24 17:58:02--  https://raw.githubusercontent.com/MasteriNeuron/datasets/main/googleplaystore.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1360155 (1.3M) [text/plain]\n",
            "Saving to: ‘googleplaystore.csv.1’\n",
            "\n",
            "\rgoogleplaystore.csv   0%[                    ]       0  --.-KB/s               \rgoogleplaystore.csv 100%[===================>]   1.30M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-10-24 17:58:02 (23.3 MB/s) - ‘googleplaystore.csv.1’ saved [1360155/1360155]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8: Titanic Dataset\n",
        "#a) Compare the survival rates based on passenger class (Pclass). Which class had the highest\n",
        "#survival rate, and why do you think that happened?\n",
        "#b) Analyze how age (Age) affected survival. Group passengers into children (Age < 18) and\n",
        "#adults (Age ≥ 18). Did children have a better chance of survival?\n",
        "#Dataset: https://github.com/MasteriNeuron/datasets.git\n",
        "#(Include your Python code and output in the code box below.)\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Titanic dataset\n",
        "df = pd.read_csv(\"titanic.csv\")\n",
        "\n",
        "# Inspect the data to ensure correct loading and check for missing values\n",
        "print(df.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
        "print(df.info())\n",
        "\n",
        "# --- Analysis for Part a) Pclass Survival Rate ---\n",
        "# Group by Pclass and calculate the mean of 'Survived' (survival rate)\n",
        "pclass_survival = df.groupby('Pclass')['Survived'].mean().sort_values(ascending=False)\n",
        "\n",
        "# --- Analysis for Part b) Age Survival Rate ---\n",
        "\n",
        "# Drop rows with missing 'Age' for clean comparison in part b\n",
        "df_age_analysis = df.dropna(subset=['Age']).copy()\n",
        "\n",
        "# Create the age group column\n",
        "df_age_analysis['Age_Group'] = df_age_analysis['Age'].apply(lambda x: 'Child' if x < 18 else 'Adult')\n",
        "\n",
        "# Group by Age_Group and calculate the mean of 'Survived'\n",
        "age_group_survival = df_age_analysis.groupby('Age_Group')['Survived'].mean().sort_values(ascending=False)\n",
        "\n",
        "print(\"\\n--- a) Survival Rate by Passenger Class (Pclass) ---\")\n",
        "print(pclass_survival.to_markdown(numalign=\"left\", stralign=\"left\"))\n",
        "print(\"\\n--- b) Survival Rate by Age Group (Children < 18 vs Adults >= 18) ---\")\n",
        "print(age_group_survival.to_markdown(numalign=\"left\", stralign=\"left\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "KhtDjDSUXTSh",
        "outputId": "085f484e-21eb-484e-b7cd-73a9d1b7bb13"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'titanic.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1711741709.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Load the Titanic dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"titanic.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Inspect the data to ensure correct loading and check for missing values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'titanic.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a67e1a1f",
        "outputId": "833a2c39-efaa-473d-f9bd-4dc9b8c6afe6"
      },
      "source": [
        "# Fetch the Titanic dataset from the GitHub link\n",
        "!wget https://raw.githubusercontent.com/MasteriNeuron/datasets/main/titanic.csv"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-24 18:04:52--  https://raw.githubusercontent.com/MasteriNeuron/datasets/main/titanic.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 60302 (59K) [text/plain]\n",
            "Saving to: ‘titanic.csv’\n",
            "\n",
            "\rtitanic.csv           0%[                    ]       0  --.-KB/s               \rtitanic.csv         100%[===================>]  58.89K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-10-24 18:04:52 (4.86 MB/s) - ‘titanic.csv’ saved [60302/60302]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6394316",
        "outputId": "0d654f68-22c0-4cf7-b87b-d4405d5c80cd"
      },
      "source": [
        "#Question 8: Titanic Dataset\n",
        "#a) Compare the survival rates based on passenger class (Pclass). Which class had the highest\n",
        "#survival rate, and why do you think that happened?\n",
        "#b) Analyze how age (Age) affected survival. Group passengers into children (Age < 18) and\n",
        "#adults (Age ≥ 18). Did children have a better chance of survival?\n",
        "#Dataset: https://github.com/MasteriNeuron/datasets.git\n",
        "#(Include your Python code and output in the code box below.)\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Titanic dataset\n",
        "df = pd.read_csv(\"titanic.csv\")\n",
        "\n",
        "# Inspect the data to ensure correct loading and check for missing values\n",
        "print(df.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
        "print(df.info())\n",
        "\n",
        "# --- Analysis for Part a) Pclass Survival Rate ---\n",
        "# Group by Pclass and calculate the mean of 'Survived' (survival rate)\n",
        "pclass_survival = df.groupby('Pclass')['Survived'].mean().sort_values(ascending=False)\n",
        "\n",
        "# --- Analysis for Part b) Age Survival Rate ---\n",
        "\n",
        "# Drop rows with missing 'Age' for clean comparison in part b\n",
        "df_age_analysis = df.dropna(subset=['Age']).copy()\n",
        "\n",
        "# Create the age group column\n",
        "df_age_analysis['Age_Group'] = df_age_analysis['Age'].apply(lambda x: 'Child' if x < 18 else 'Adult')\n",
        "\n",
        "# Group by Age_Group and calculate the mean of 'Survived'\n",
        "age_group_survival = df_age_analysis.groupby('Age_Group')['Survived'].mean().sort_values(ascending=False)\n",
        "\n",
        "print(\"\\n--- a) Survival Rate by Passenger Class (Pclass) ---\")\n",
        "print(pclass_survival.to_markdown(numalign=\"left\", stralign=\"left\"))\n",
        "print(\"\\n--- b) Survival Rate by Age Group (Children < 18 vs Adults >= 18) ---\")\n",
        "print(age_group_survival.to_markdown(numalign=\"left\", stralign=\"left\"))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| PassengerId   | Survived   | Pclass   | Name                                                | Sex    | Age   | SibSp   | Parch   | Ticket           | Fare    | Cabin   | Embarked   |\n",
            "|:--------------|:-----------|:---------|:----------------------------------------------------|:-------|:------|:--------|:--------|:-----------------|:--------|:--------|:-----------|\n",
            "| 1             | 0          | 3        | Braund, Mr. Owen Harris                             | male   | 22    | 1       | 0       | A/5 21171        | 7.25    | nan     | S          |\n",
            "| 2             | 1          | 1        | Cumings, Mrs. John Bradley (Florence Briggs Thayer) | female | 38    | 1       | 0       | PC 17599         | 71.2833 | C85     | C          |\n",
            "| 3             | 1          | 3        | Heikkinen, Miss. Laina                              | female | 26    | 0       | 0       | STON/O2. 3101282 | 7.925   | nan     | S          |\n",
            "| 4             | 1          | 1        | Futrelle, Mrs. Jacques Heath (Lily May Peel)        | female | 35    | 1       | 0       | 113803           | 53.1    | C123    | S          |\n",
            "| 5             | 0          | 3        | Allen, Mr. William Henry                            | male   | 35    | 0       | 0       | 373450           | 8.05    | nan     | S          |\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 891 entries, 0 to 890\n",
            "Data columns (total 12 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   PassengerId  891 non-null    int64  \n",
            " 1   Survived     891 non-null    int64  \n",
            " 2   Pclass       891 non-null    int64  \n",
            " 3   Name         891 non-null    object \n",
            " 4   Sex          891 non-null    object \n",
            " 5   Age          714 non-null    float64\n",
            " 6   SibSp        891 non-null    int64  \n",
            " 7   Parch        891 non-null    int64  \n",
            " 8   Ticket       891 non-null    object \n",
            " 9   Fare         891 non-null    float64\n",
            " 10  Cabin        204 non-null    object \n",
            " 11  Embarked     889 non-null    object \n",
            "dtypes: float64(2), int64(5), object(5)\n",
            "memory usage: 83.7+ KB\n",
            "None\n",
            "\n",
            "--- a) Survival Rate by Passenger Class (Pclass) ---\n",
            "| Pclass   | Survived   |\n",
            "|:---------|:-----------|\n",
            "| 1        | 0.62963    |\n",
            "| 2        | 0.472826   |\n",
            "| 3        | 0.242363   |\n",
            "\n",
            "--- b) Survival Rate by Age Group (Children < 18 vs Adults >= 18) ---\n",
            "| Age_Group   | Survived   |\n",
            "|:------------|:-----------|\n",
            "| Child       | 0.539823   |\n",
            "| Adult       | 0.381032   |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9: Flight Price Prediction Dataset\n",
        "#a) How do flight prices vary with the days left until departure? Identify any exponential price\n",
        "#surges and recommend the best booking window.\n",
        "#b)Compare prices across airlines for the same route (e.g., Delhi-Mumbai). Which airlines are\n",
        "#consistently cheaper/premium, and why?\n",
        "#Dataset: https://github.com/MasteriNeuron/datasets.git\n",
        "#(Include your Python code and output in the code box below.)\n",
        "\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"flight_price.csv\")\n",
        "\n",
        "# --- Part a) Price vs. Days Left ---\n",
        "\n",
        "# To get a clear trend, we group by 'days_left' and 'class' and calculate the mean price\n",
        "days_vs_price = df.groupby(['days_left', 'class'])['price'].mean().reset_index()\n",
        "\n",
        "# Create the line chart to visualize the trend\n",
        "chart_days_left = alt.Chart(days_vs_price).mark_line(point=True).encode(\n",
        "    x=alt.X('days_left', title='Days Left Until Departure'),\n",
        "    y=alt.Y('price', title='Average Price (INR)'),\n",
        "    color='class:N', # 'N' for nominal (categorical)\n",
        "    tooltip=['days_left', 'class', alt.Tooltip('price', format=\".0f\")]\n",
        ").properties(\n",
        "    title='Average Flight Price by Days Left and Class',\n",
        ").interactive()\n",
        "\n",
        "# Save the chart\n",
        "chart_days_left.save('flight_price_vs_days_left.json')\n",
        "\n",
        "# Print summary data to show the surge\n",
        "print(\"\\n--- Price Surge Analysis (Economy Class) ---\")\n",
        "print(\"Average Price when booking 40-49 days out (Economy):\")\n",
        "print(days_vs_price[(days_vs_price['class'] == 'Economy') & (days_vs_price['days_left'] >= 40)].head(10).to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
        "print(\"\\nAverage Price when booking 1-10 days out (Economy):\")\n",
        "print(days_vs_price[(days_vs_price['class'] == 'Economy') & (days_vs_price['days_left'] <= 10)].head(10).to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "# --- Part b) Airline Comparison (Delhi-Mumbai, Economy) ---\n",
        "\n",
        "# Filter for Economy class on the specified route\n",
        "df_del_mum_econ = df[\n",
        "    (df['source_city'] == 'Delhi') &\n",
        "    (df['destination_city'] == 'Mumbai') &\n",
        "    (df['class'] == 'Economy')\n",
        "]\n",
        "\n",
        "# Calculate average price by airline\n",
        "airline_comparison = df_del_mum_econ.groupby('airline')['price'].mean().sort_values().reset_index()\n",
        "\n",
        "print(\"\\n--- Average Price for Delhi-Mumbai (Economy Class) ---\")\n",
        "print(airline_comparison.to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "# Create the bar chart\n",
        "chart_airline = alt.Chart(airline_comparison).mark_bar().encode(\n",
        "    # Sort bars in descending order of price\n",
        "    x=alt.X('price', title='Average Price (INR)'),\n",
        "    y=alt.Y('airline', title='Airline', sort='-x')\n",
        ").properties(\n",
        "    title='Average Flight Price (Economy) for Delhi to Mumbai'\n",
        ").interactive()\n",
        "\n",
        "# Save the chart\n",
        "chart_airline.save('delhi_mumbai_airline_prices.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "ba9gaCWeZYpl",
        "outputId": "ef2eab99-a40f-4433-9699-c3d9dc9067b9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'flight_price.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1493835788.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"flight_price.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# --- Part a) Price vs. Days Left ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'flight_price.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67d6e14d",
        "outputId": "e7d124c7-2170-4a30-b2b2-0bc4ca7b351a"
      },
      "source": [
        "# Fetch the Flight Price Prediction dataset from the GitHub link\n",
        "!wget https://raw.githubusercontent.com/MasteriNeuron/datasets/main/flight_price.csv"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-24 18:12:08--  https://raw.githubusercontent.com/MasteriNeuron/datasets/main/flight_price.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24683279 (24M) [text/plain]\n",
            "Saving to: ‘flight_price.csv’\n",
            "\n",
            "flight_price.csv    100%[===================>]  23.54M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-10-24 18:12:08 (168 MB/s) - ‘flight_price.csv’ saved [24683279/24683279]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93486666",
        "outputId": "249db1f0-a216-4a32-b135-c0240c0cacb1"
      },
      "source": [
        "#Question 9: Flight Price Prediction Dataset\n",
        "#a) How do flight prices vary with the days left until departure? Identify any exponential price\n",
        "#surges and recommend the best booking window.\n",
        "#b)Compare prices across airlines for the same route (e.g., Delhi-Mumbai). Which airlines are\n",
        "#consistently cheaper/premium, and why?\n",
        "#Dataset: https://github.com/MasteriNeuron/datasets.git\n",
        "#(Include your Python code and output in the code box below.)\n",
        "\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"flight_price.csv\")\n",
        "\n",
        "# --- Part a) Price vs. Days Left ---\n",
        "\n",
        "# To get a clear trend, we group by 'days_left' and 'class' and calculate the mean price\n",
        "days_vs_price = df.groupby(['days_left', 'class'])['price'].mean().reset_index()\n",
        "\n",
        "# Create the line chart to visualize the trend\n",
        "chart_days_left = alt.Chart(days_vs_price).mark_line(point=True).encode(\n",
        "    x=alt.X('days_left', title='Days Left Until Departure'),\n",
        "    y=alt.Y('price', title='Average Price (INR)'),\n",
        "    color='class:N', # 'N' for nominal (categorical)\n",
        "    tooltip=['days_left', 'class', alt.Tooltip('price', format=\".0f\")]\n",
        ").properties(\n",
        "    title='Average Flight Price by Days Left and Class',\n",
        ").interactive()\n",
        "\n",
        "# Save the chart\n",
        "chart_days_left.save('flight_price_vs_days_left.json')\n",
        "\n",
        "# Print summary data to show the surge\n",
        "print(\"\\n--- Price Surge Analysis (Economy Class) ---\")\n",
        "print(\"Average Price when booking 40-49 days out (Economy):\")\n",
        "print(days_vs_price[(days_vs_price['class'] == 'Economy') & (days_vs_price['days_left'] >= 40)].head(10).to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
        "print(\"\\nAverage Price when booking 1-10 days out (Economy):\")\n",
        "print(days_vs_price[(days_vs_price['class'] == 'Economy') & (days_vs_price['days_left'] <= 10)].head(10).to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "# --- Part b) Airline Comparison (Delhi-Mumbai, Economy) ---\n",
        "\n",
        "# Filter for Economy class on the specified route\n",
        "df_del_mum_econ = df[\n",
        "    (df['source_city'] == 'Delhi') &\n",
        "    (df['destination_city'] == 'Mumbai') &\n",
        "    (df['class'] == 'Economy')\n",
        "]\n",
        "\n",
        "# Calculate average price by airline\n",
        "airline_comparison = df_del_mum_econ.groupby('airline')['price'].mean().sort_values().reset_index()\n",
        "\n",
        "print(\"\\n--- Average Price for Delhi-Mumbai (Economy Class) ---\")\n",
        "print(airline_comparison.to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "# Create the bar chart\n",
        "chart_airline = alt.Chart(airline_comparison).mark_bar().encode(\n",
        "    # Sort bars in descending order of price\n",
        "    x=alt.X('price', title='Average Price (INR)'),\n",
        "    y=alt.Y('airline', title='Airline', sort='-x')\n",
        ").properties(\n",
        "    title='Average Flight Price (Economy) for Delhi to Mumbai'\n",
        ").interactive()\n",
        "\n",
        "# Save the chart\n",
        "chart_airline.save('delhi_mumbai_airline_prices.json')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Price Surge Analysis (Economy Class) ---\n",
            "Average Price when booking 40-49 days out (Economy):\n",
            "| days_left   | class   | price   |\n",
            "|:------------|:--------|:--------|\n",
            "| 40          | Economy | 4739.73 |\n",
            "| 41          | Economy | 4774.03 |\n",
            "| 42          | Economy | 4783.83 |\n",
            "| 43          | Economy | 4893.28 |\n",
            "| 44          | Economy | 4890.23 |\n",
            "| 45          | Economy | 4908.19 |\n",
            "| 46          | Economy | 4704.5  |\n",
            "| 47          | Economy | 4669.65 |\n",
            "| 48          | Economy | 4717.69 |\n",
            "| 49          | Economy | 4750.81 |\n",
            "\n",
            "Average Price when booking 1-10 days out (Economy):\n",
            "| days_left   | class   | price   |\n",
            "|:------------|:--------|:--------|\n",
            "| 1           | Economy | 14613.2 |\n",
            "| 2           | Economy | 13980.8 |\n",
            "| 3           | Economy | 13174.1 |\n",
            "| 4           | Economy | 10901.4 |\n",
            "| 5           | Economy | 10605.9 |\n",
            "| 6           | Economy | 10319.7 |\n",
            "| 7           | Economy | 10471.9 |\n",
            "| 8           | Economy | 10479.5 |\n",
            "| 9           | Economy | 11352.9 |\n",
            "| 10          | Economy | 11187.5 |\n",
            "\n",
            "--- Average Price for Delhi-Mumbai (Economy Class) ---\n",
            "| airline   | price   |\n",
            "|:----------|:--------|\n",
            "| AirAsia   | 3981.19 |\n",
            "| Indigo    | 4473.74 |\n",
            "| SpiceJet  | 4628.25 |\n",
            "| GO_FIRST  | 5762.21 |\n",
            "| Vistara   | 6983.66 |\n",
            "| Air_India | 6996.98 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 10: HR Analytics Dataset\n",
        "#a). What factors most strongly correlate with employee attrition? Use visualizations to show key\n",
        "#drivers (e.g., satisfaction, overtime, salary).\n",
        "#b). Are employees with more projects more likely to leave?\n",
        "#Dataset: hr_analytics\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"hr_analytics.csv\")\n",
        "\n",
        "# Inspect data\n",
        "print(\"--- Initial Data Head ---\")\n",
        "print(df.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
        "print(\"\\n--- Initial Data Info ---\")\n",
        "df.info()\n",
        "\n",
        "# --- Part a) Correlation and Key Drivers ---\n",
        "\n",
        "# Create a copy for correlation analysis\n",
        "df_corr = df.copy()\n",
        "\n",
        "# Convert 'salary' to a numeric, ordinal variable for correlation\n",
        "salary_map = {'low': 0, 'medium': 1, 'high': 2}\n",
        "df_corr['salary_numeric'] = df_corr['salary'].map(salary_map)\n",
        "\n",
        "# Select only numeric columns for the correlation matrix\n",
        "numeric_df = df_corr.select_dtypes(include=['number'])\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "corr_matrix = numeric_df.corr()\n",
        "\n",
        "# Get the correlation of all factors with 'left' (attrition)\n",
        "print(\"\\n--- a) Correlation with Employee Attrition ('left') ---\")\n",
        "left_corr = corr_matrix[['left']].sort_values(by='left', ascending=False)\n",
        "print(left_corr.to_markdown(numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "# Visualization 1: Correlation Heatmap\n",
        "# Melt the matrix for Altair\n",
        "corr_data = corr_matrix.stack().reset_index(name='correlation')\n",
        "corr_data.columns = ['Feature_1', 'Feature_2', 'Correlation'] # Rename for clarity\n",
        "\n",
        "base = alt.Chart(corr_data).encode(\n",
        "    x=alt.X('Feature_1', title='Feature'),\n",
        "    y=alt.Y('Feature_2', title='Feature'),\n",
        "    tooltip=['Feature_1', 'Feature_2', alt.Tooltip('Correlation', format='.3f')]\n",
        ")\n",
        "\n",
        "# The heatmap\n",
        "heatmap = base.mark_rect().encode(\n",
        "    color=alt.Color('Correlation',\n",
        "                    scale=alt.Scale(range='diverging', domain=[-1, 0, 1]),\n",
        "                    legend=alt.Legend(title=\"Correlation\"))\n",
        ")\n",
        "\n",
        "# Add text labels\n",
        "text = base.mark_text().encode(\n",
        "    text=alt.Text('Correlation', format='.2f'),\n",
        "    color=alt.value('black') # Make text readable\n",
        ")\n",
        "\n",
        "chart_heatmap = (heatmap + text).properties(\n",
        "    title='Correlation Matrix of HR Analytics Factors',\n",
        "    width=500,\n",
        "    height=500\n",
        ").interactive()\n",
        "\n",
        "chart_heatmap.save('hr_correlation_heatmap.json')\n",
        "\n",
        "\n",
        "# Visualization 2: Satisfaction Level vs. Attrition\n",
        "# This visualization shows the *distribution* of satisfaction for those who left vs. stayed\n",
        "chart_satisfaction = alt.Chart(df).transform_density(\n",
        "    'satisfaction_level',\n",
        "    as_=['satisfaction', 'density'], # 'satisfaction' and 'density' are the new columns\n",
        "    groupby=['left']\n",
        ").mark_area(opacity=0.5).encode(\n",
        "    x=alt.X('satisfaction:Q', title='Satisfaction Level'),\n",
        "    y=alt.Y('density:Q', title='Density'),\n",
        "    # Corrected: Removed the invalid 'labels' parameter from alt.Legend\n",
        "    color=alt.Color('left:N', legend=alt.Legend(title=\"Employee Status (0=Stayed, 1=Left)\")),\n",
        "    tooltip=['left', 'satisfaction:Q', 'density:Q']\n",
        ").properties(\n",
        "    title='Employee Attrition by Satisfaction Level'\n",
        ").interactive()\n",
        "\n",
        "chart_satisfaction.save('hr_satisfaction_density.json')\n",
        "\n",
        "# Visualization 3: Salary vs. Attrition Rate\n",
        "# This visualization shows the *rate* of attrition for each salary level\n",
        "salary_attrition = df.groupby('salary')['left'].mean().reset_index()\n",
        "\n",
        "chart_salary = alt.Chart(salary_attrition).mark_bar().encode(\n",
        "    x=alt.X('salary', title='Salary Level', sort=['low', 'medium', 'high']),\n",
        "    y=alt.Y('left', title='Attrition Rate', axis=alt.Axis(format='%')),\n",
        "    tooltip=['salary', alt.Tooltip('left', format='.1%')]\n",
        ").properties(\n",
        "    title='Attrition Rate by Salary Level'\n",
        ")\n",
        "\n",
        "chart_salary.save('hr_salary_attrition.json')\n",
        "\n",
        "\n",
        "# --- Part b) Number of Projects vs. Attrition ---\n",
        "\n",
        "# Group by 'number_project' and calculate the mean of 'left' (attrition rate)\n",
        "project_attrition = df.groupby('number_project')['left'].mean().reset_index()\n",
        "\n",
        "print(\"\\n--- b) Attrition Rate by Number of Projects ---\")\n",
        "print(project_attrition.to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "# Visualization 4: Project Count vs. Attrition Rate\n",
        "chart_projects = alt.Chart(project_attrition).mark_bar().encode(\n",
        "    # Use ':O' to treat the number as an ordinal (categorical) axis\n",
        "    x=alt.X('number_project:O', title='Number of Projects'),\n",
        "    y=alt.Y('left', title='Attrition Rate', axis=alt.Axis(format='%')),\n",
        "    tooltip=['number_project', alt.Tooltip('left', format='.1%')]\n",
        ").properties(\n",
        "    title='Attrition Rate by Number of Projects'\n",
        ")\n",
        "\n",
        "chart_projects.save('hr_project_attrition.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "zpT3cBuGbCJt",
        "outputId": "6bb255e8-bc7c-49c7-c024-3ae996c949ef"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'hr_analytics.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1145774758.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hr_analytics.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Inspect data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hr_analytics.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97c1f4e6",
        "outputId": "72e05711-a336-43e7-c910-97dfb357b981"
      },
      "source": [
        "# Fetch the HR Analytics dataset from the GitHub link\n",
        "!wget https://raw.githubusercontent.com/MasteriNeuron/datasets/main/hr_analytics.csv"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-24 18:18:01--  https://raw.githubusercontent.com/MasteriNeuron/datasets/main/hr_analytics.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 566778 (553K) [text/plain]\n",
            "Saving to: ‘hr_analytics.csv’\n",
            "\n",
            "hr_analytics.csv    100%[===================>] 553.49K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-10-24 18:18:01 (15.3 MB/s) - ‘hr_analytics.csv’ saved [566778/566778]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dda8906",
        "outputId": "ffed2182-1903-41b8-d6b3-57b241f07917"
      },
      "source": [
        "#Question 10: HR Analytics Dataset\n",
        "#a). What factors most strongly correlate with employee attrition? Use visualizations to show key\n",
        "#drivers (e.g., satisfaction, overtime, salary).\n",
        "#b). Are employees with more projects more likely to leave?\n",
        "#Dataset: hr_analytics\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"hr_analytics.csv\")\n",
        "\n",
        "# Inspect data\n",
        "print(\"--- Initial Data Head ---\")\n",
        "print(df.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
        "print(\"\\n--- Initial Data Info ---\")\n",
        "df.info()\n",
        "\n",
        "# --- Part a) Correlation and Key Drivers ---\n",
        "\n",
        "# Create a copy for correlation analysis\n",
        "df_corr = df.copy()\n",
        "\n",
        "# Convert 'salary' to a numeric, ordinal variable for correlation\n",
        "salary_map = {'low': 0, 'medium': 1, 'high': 2}\n",
        "df_corr['salary_numeric'] = df_corr['salary'].map(salary_map)\n",
        "\n",
        "# Select only numeric columns for the correlation matrix\n",
        "numeric_df = df_corr.select_dtypes(include=['number'])\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "corr_matrix = numeric_df.corr()\n",
        "\n",
        "# Get the correlation of all factors with 'left' (attrition)\n",
        "print(\"\\n--- a) Correlation with Employee Attrition ('left') ---\")\n",
        "left_corr = corr_matrix[['left']].sort_values(by='left', ascending=False)\n",
        "print(left_corr.to_markdown(numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "# Visualization 1: Correlation Heatmap\n",
        "# Melt the matrix for Altair\n",
        "corr_data = corr_matrix.stack().reset_index(name='correlation')\n",
        "corr_data.columns = ['Feature_1', 'Feature_2', 'Correlation'] # Rename for clarity\n",
        "\n",
        "base = alt.Chart(corr_data).encode(\n",
        "    x=alt.X('Feature_1', title='Feature'),\n",
        "    y=alt.Y('Feature_2', title='Feature'),\n",
        "    tooltip=['Feature_1', 'Feature_2', alt.Tooltip('Correlation', format='.3f')]\n",
        ")\n",
        "\n",
        "# The heatmap\n",
        "heatmap = base.mark_rect().encode(\n",
        "    color=alt.Color('Correlation',\n",
        "                    scale=alt.Scale(range='diverging', domain=[-1, 0, 1]),\n",
        "                    legend=alt.Legend(title=\"Correlation\"))\n",
        ")\n",
        "\n",
        "# Add text labels\n",
        "text = base.mark_text().encode(\n",
        "    text=alt.Text('Correlation', format='.2f'),\n",
        "    color=alt.value('black') # Make text readable\n",
        ")\n",
        "\n",
        "chart_heatmap = (heatmap + text).properties(\n",
        "    title='Correlation Matrix of HR Analytics Factors',\n",
        "    width=500,\n",
        "    height=500\n",
        ").interactive()\n",
        "\n",
        "chart_heatmap.save('hr_correlation_heatmap.json')\n",
        "\n",
        "\n",
        "# Visualization 2: Satisfaction Level vs. Attrition\n",
        "# This visualization shows the *distribution* of satisfaction for those who left vs. stayed\n",
        "chart_satisfaction = alt.Chart(df).transform_density(\n",
        "    'satisfaction_level',\n",
        "    as_=['satisfaction', 'density'], # 'satisfaction' and 'density' are the new columns\n",
        "    groupby=['left']\n",
        ").mark_area(opacity=0.5).encode(\n",
        "    x=alt.X('satisfaction:Q', title='Satisfaction Level'),\n",
        "    y=alt.Y('density:Q', title='Density'),\n",
        "    # Corrected: Removed the invalid 'labels' parameter from alt.Legend\n",
        "    color=alt.Color('left:N', legend=alt.Legend(title=\"Employee Status (0=Stayed, 1=Left)\")),\n",
        "    tooltip=['left', 'satisfaction:Q', 'density:Q']\n",
        ").properties(\n",
        "    title='Employee Attrition by Satisfaction Level'\n",
        ").interactive()\n",
        "\n",
        "chart_satisfaction.save('hr_satisfaction_density.json')\n",
        "\n",
        "# Visualization 3: Salary vs. Attrition Rate\n",
        "# This visualization shows the *rate* of attrition for each salary level\n",
        "salary_attrition = df.groupby('salary')['left'].mean().reset_index()\n",
        "\n",
        "chart_salary = alt.Chart(salary_attrition).mark_bar().encode(\n",
        "    x=alt.X('salary', title='Salary Level', sort=['low', 'medium', 'high']),\n",
        "    y=alt.Y('left', title='Attrition Rate', axis=alt.Axis(format='%')),\n",
        "    tooltip=['salary', alt.Tooltip('left', format='.1%')]\n",
        ").properties(\n",
        "    title='Attrition Rate by Salary Level'\n",
        ")\n",
        "\n",
        "chart_salary.save('hr_salary_attrition.json')\n",
        "\n",
        "\n",
        "# --- Part b) Number of Projects vs. Attrition ---\n",
        "\n",
        "# Group by 'number_project' and calculate the mean of 'left' (attrition rate)\n",
        "project_attrition = df.groupby('number_project')['left'].mean().reset_index()\n",
        "\n",
        "print(\"\\n--- b) Attrition Rate by Number of Projects ---\")\n",
        "print(project_attrition.to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "# Visualization 4: Project Count vs. Attrition Rate\n",
        "chart_projects = alt.Chart(project_attrition).mark_bar().encode(\n",
        "    # Use ':O' to treat the number as an ordinal (categorical) axis\n",
        "    x=alt.X('number_project:O', title='Number of Projects'),\n",
        "    y=alt.Y('left', title='Attrition Rate', axis=alt.Axis(format='%')),\n",
        "    tooltip=['number_project', alt.Tooltip('left', format='.1%')]\n",
        ").properties(\n",
        "    title='Attrition Rate by Number of Projects'\n",
        ")\n",
        "\n",
        "chart_projects.save('hr_project_attrition.json')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initial Data Head ---\n",
            "| satisfaction_level   | last_evaluation   | number_project   | average_montly_hours   | time_spend_company   | Work_accident   | left   | promotion_last_5years   | sales   | salary   |\n",
            "|:---------------------|:------------------|:-----------------|:-----------------------|:---------------------|:----------------|:-------|:------------------------|:--------|:---------|\n",
            "| 0.38                 | 0.53              | 2                | 157                    | 3                    | 0               | 1      | 0                       | sales   | low      |\n",
            "| 0.8                  | 0.86              | 5                | 262                    | 6                    | 0               | 1      | 0                       | sales   | medium   |\n",
            "| 0.11                 | 0.88              | 7                | 272                    | 4                    | 0               | 1      | 0                       | sales   | medium   |\n",
            "| 0.72                 | 0.87              | 5                | 223                    | 5                    | 0               | 1      | 0                       | sales   | low      |\n",
            "| 0.37                 | 0.52              | 2                | 159                    | 3                    | 0               | 1      | 0                       | sales   | low      |\n",
            "\n",
            "--- Initial Data Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 14999 entries, 0 to 14998\n",
            "Data columns (total 10 columns):\n",
            " #   Column                 Non-Null Count  Dtype  \n",
            "---  ------                 --------------  -----  \n",
            " 0   satisfaction_level     14999 non-null  float64\n",
            " 1   last_evaluation        14999 non-null  float64\n",
            " 2   number_project         14999 non-null  int64  \n",
            " 3   average_montly_hours   14999 non-null  int64  \n",
            " 4   time_spend_company     14999 non-null  int64  \n",
            " 5   Work_accident          14999 non-null  int64  \n",
            " 6   left                   14999 non-null  int64  \n",
            " 7   promotion_last_5years  14999 non-null  int64  \n",
            " 8   sales                  14999 non-null  object \n",
            " 9   salary                 14999 non-null  object \n",
            "dtypes: float64(2), int64(6), object(2)\n",
            "memory usage: 1.1+ MB\n",
            "\n",
            "--- a) Correlation with Employee Attrition ('left') ---\n",
            "|                       | left       |\n",
            "|:----------------------|:-----------|\n",
            "| left                  | 1          |\n",
            "| time_spend_company    | 0.144822   |\n",
            "| average_montly_hours  | 0.0712872  |\n",
            "| number_project        | 0.0237872  |\n",
            "| last_evaluation       | 0.00656712 |\n",
            "| promotion_last_5years | -0.0617881 |\n",
            "| Work_accident         | -0.154622  |\n",
            "| salary_numeric        | -0.157898  |\n",
            "| satisfaction_level    | -0.388375  |\n",
            "\n",
            "--- b) Attrition Rate by Number of Projects ---\n",
            "| number_project   | left      |\n",
            "|:-----------------|:----------|\n",
            "| 2                | 0.656198  |\n",
            "| 3                | 0.0177559 |\n",
            "| 4                | 0.0936999 |\n",
            "| 5                | 0.221659  |\n",
            "| 6                | 0.557922  |\n",
            "| 7                | 1         |\n"
          ]
        }
      ]
    }
  ]
}